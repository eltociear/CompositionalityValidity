{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager\n",
    "from matplotlib.font_manager import findfont, FontProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line to your own path\n",
    "# Remember to run this line before any execution of the cells!\n",
    "os.environ['BASE_DIR'] = '/path/to/base_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import BASE_DIR, DATA_DIR\n",
    "from helper_utils.helper_methods import list_datasets_and_their_splits, list_hardcode_datasets_and_their_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting parameters\n",
    "font = {'family' : 'serif',\n",
    "        # 'weight' : 'bold',\n",
    "        'size'   : 11}\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "mpl.rc('font', **font)\n",
    "mpl.rc('xtick', labelsize=11) \n",
    "plt.rcParams[\"font.family\"] = \"Nimbus Roman\"\n",
    "mpl.rc('ytick', labelsize=11) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Dataset Stat\n",
    "\n",
    "Compute the basic stastics of different datasets, including number of instances, raw sequence length, lengths after tokenization, overlap between a subsample of instances.\n",
    "\n",
    "Documentation of each function can be found via `dataset_stat.py`. The functions in this section assumes access to datasets contain in `data/` dir of the BASE_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_stat import build_table_for_all_datasets, compute_sample_overlap_all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs a PrettyTable for the data type inputed; Data type = str in {num_instances, raw_avg_length, tok_seq_length, lexical_overlap}\n",
    "# sub_datatype = {input, output}, used when computing raw_avg_length and tok_seq_length.\n",
    "# model name = {HF model names with FastTokenizers}, default to t5-base. Can be facebook/bart-base; used only for computing seq length after tokenization.\n",
    "print(\"Computing number of instances\")\n",
    "build_table_for_all_datasets(\"num_instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing average length of each dataset\")\n",
    "build_table_for_all_datasets('raw_avg_length', sub_datatype='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing average length of each dataset after tokenization\")\n",
    "build_table_for_all_datasets('raw_avg_length', sub_datatype='input', model_name='t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_sample_overlap_all_datasets(lex_type=\"Lev\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training curve\n",
    "\n",
    "Plot the training curve of HF models. This section assumes access to `trained_models/` in BASE_DIR. Each model dir should include a `trainer_state.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import load_training_curve_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Curve Info\n",
    "dataset_name = 'geoquery'\n",
    "split = 'standard'\n",
    "model_name = 't5-base'\n",
    "dataset_names, splits_mapping = list_hardcode_datasets_and_their_splits()\n",
    "\n",
    "steps, ems, best_em = load_training_curve_info(model_name, dataset_name, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve(steps, ems, best_em=-1):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    ax.plot(steps, ems, label=model_name)\n",
    "    if best_em != -1:\n",
    "        ax.plot([steps[0], steps[-1]], [best_em, best_em], label=\"Best EM\")\n",
    "\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('EM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curve(steps, ems, best_em)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training curve for all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import analysis_utils\n",
    "importlib.reload(analysis_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#D81B60', '#999999', '#7570B3', '#E66100', '#7570B3', '#E66100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve for all splits\n",
    "dataset_name = 'geoquery'\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "splits = []\n",
    "for idx, split in enumerate(splits_mapping[dataset_name]):\n",
    "    # steps, ems, best_em = analysis_utils.load_training_curve_info(model_name, dataset_name, split, checkpoint=None)\n",
    "    steps, ems, best_em = analysis_utils.load_avg_training_curve_info(model_name, dataset_name, split, checkpoint=None)\n",
    "    # if split == 'standard':\n",
    "    #     steps = steps[:638]\n",
    "    #     ems = ems[:638]\n",
    "    ax.plot(steps, ems, label=split, color=colors[idx], alpha=0.9, linewidth=2.0)\n",
    "    splits.append(split)\n",
    "    \n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('EM')\n",
    "ax.grid(alpha=0.4)\n",
    "f = lambda m,c: plt.plot([],[],marker=m, color=c, ls=\"none\")[0]\n",
    "handles = [f('s', colors[idx]) for idx in range(len(splits))]\n",
    "labels = [split for split in splits]\n",
    "ax.legend(handles, labels)\n",
    "fig.suptitle(\"Training Curve of \" + model_name + \" on \" + dataset_name)\n",
    "plt.savefig(f\"{BASE_DIR}/results/analysis_res/{model_name}-{dataset_name}.pdf\", format='pdf', bbox_inches=\"tight\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluates the models and save to csv files. This section assumes access to `pred/` dir, which includes `.txt` files of model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_utils import evaluate_model, evaluate_all_model_for_dataset, evaluate_all, gen_performance_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dataset_name='geoquery', split='standard', model_name='t5-base', random_seed='42', eval_split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time, do not run\n",
    "res = evaluate_all_model_for_dataset(dataset_name='geoquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on all datasets, output will include different random seeds and avg/std\n",
    "# Takes a long time, do not run unless  \n",
    "res = evaluate_all()\n",
    "res.to_csv(os.getenv('BASE_DIR') + '/results/exact_match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance table, in which the numbers are averaged across random seeds\n",
    "res_table = pd.read_csv(os.getenv('BASE_DIR') + '/results/exact_match.csv')\n",
    "columns_to_keep = ['raw_exact_match', 'ignore_space', 'f1']\n",
    "res = gen_performance_table(columns_to_keep, res_table)\n",
    "res.to_csv(os.getenv('BASE_DIR') + '/results/perf_table.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Concurrence\n",
    "\n",
    "This section can be ran without access to `data/`. Make sure to include `results/exact_match.csv` to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time to run, can just use the pre-computed performance table\n",
    "# cogs_perf = evaluate_all_model_for_dataset('COGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_table = pd.read_csv(BASE_DIR + '/results/exact_match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import analysis_utils\n",
    "reload(analysis_utils)\n",
    "from analysis_utils import compute_concurrence, compute_concurr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(compute_concurrence(perf_table, \"COGS\", \"COGS\", \"random_cvcv\", \"random_cvcv\"))\n",
    "print(compute_concurrence(perf_table, \"geoquery\", \"geoquery\", \"tmcd_random_cvcv\", \"tmcd_random_cvcv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity checks\n",
    "print(compute_concurrence(perf_table, \"COGS\", \"SCAN\", \"random_cvcv\", \"addprim_jump\"))\n",
    "print(compute_concurrence(perf_table, \"COGS\", \"SCAN\", \"no_mod\", \"addprim_jump\"))\n",
    "print(compute_concurrence(perf_table, \"COGS\", \"geoquery\", \"no_mod\", \"standard\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute concurrence between all datasets and their splits\n",
    "# Use metric_type to indicate the metric to compute concurrence\n",
    "# There will be some None entries, because the training are not done for all models.\n",
    "concurrences = compute_concurr_all(metric_type='ignore_space', corref='Kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the concurrences to file\n",
    "concurrences.to_csv(os.getenv('BASE_DIR') + '/results/Kendall_concurrences.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d93c15ccce56fd147282e5a685498c153716f1ad1db6309375a6b8c6a72806b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
